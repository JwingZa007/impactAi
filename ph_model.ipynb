{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T12:22:56.050749Z",
     "start_time": "2025-03-13T12:21:54.969655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# ------------------------------\n",
    "# 1. 数据读取与基本预处理\n",
    "# ------------------------------\n",
    "file_path = 'IMPACT.sensors.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 确保有时间戳列并转换为 datetime 类型\n",
    "timestamp_column = 'createdAt'\n",
    "data[timestamp_column] = pd.to_datetime(data[timestamp_column])\n",
    "\n",
    "# 按时间排序\n",
    "data = data.sort_values(by=timestamp_column).reset_index(drop=True)\n",
    "\n",
    "# 简单检查数据缺失情况（可根据需要做插值或去除）\n",
    "print(\"缺失情况：\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# 演示插值：若缺失较少或轻微，可以用以下方式\n",
    "# data = data.interpolate(method='time')  # 基于时间序列插值\n",
    "\n",
    "# ------------------------------\n",
    "# 2. 特征选择与生成\n",
    "#    - 在 pH 基础上添加温度 temperature\n",
    "#    - 构造时间特征 hour, day_of_week, is_weekend\n",
    "# ------------------------------\n",
    "# 可以根据实际传感器数据列名进行调整\n",
    "features = ['pH', 'temperature']  # 如果只想加温度，可保留这两个；有更多传感器，可一并加入\n",
    "\n",
    "# 提取必要列\n",
    "data_features = data[features].copy()\n",
    "\n",
    "# 生成时间特征\n",
    "data['hour'] = data[timestamp_column].dt.hour\n",
    "data['day_of_week'] = data[timestamp_column].dt.dayofweek\n",
    "data['is_weekend'] = data['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "# 将时间特征也加入到特征列表中\n",
    "time_features = ['hour', 'day_of_week', 'is_weekend']\n",
    "data_features[time_features] = data[time_features]\n",
    "\n",
    "# 查看特征相关性（可根据相关性筛选特征）\n",
    "corr_matrix = data_features.corr()\n",
    "print(\"\\n特征相关性：\")\n",
    "print(corr_matrix)\n",
    "\n",
    "# 选取最终特征\n",
    "final_features = features + time_features\n",
    "\n",
    "# ------------------------------\n",
    "# 3. 数据归一化\n",
    "# ------------------------------\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = scaler.fit_transform(data_features[final_features])  # 按列归一化\n",
    "\n",
    "# ------------------------------\n",
    "# 4. 构建序列数据（含滑窗步长）\n",
    "#    - sequence_length: 过去多少个时间步\n",
    "#    - prediction_steps: 要预测哪些未来点\n",
    "#    - step_size: 滑窗移动步长（原为1，这里设6，可减少样本相关性）\n",
    "# ------------------------------\n",
    "\n",
    "sequence_length = 12  # 过去 2 个时间步（可视为 12*10min = 120min，例如）\n",
    "prediction_steps = [6, 12, 18, 24, 30, 36, 42, 48]  # 预测未来 1~8 小时(以10min为间隔)\n",
    "step_size = 6  # 滑窗步长\n",
    "\n",
    "def create_future_sequences(data_array, sequence_length, prediction_steps, step_size=6):\n",
    "    X, y = [], []\n",
    "    max_step = max(prediction_steps)\n",
    "\n",
    "    for i in range(0, len(data_array) - sequence_length - max_step, step_size):\n",
    "        # 取过去 sequence_length 长度的数据作为X\n",
    "        X.append(data_array[i : i + sequence_length])\n",
    "\n",
    "        # 对于每个预测步长，取后续几个点的平均（或其他统计方式）作为标签\n",
    "        future_values = [\n",
    "            np.mean(data_array[i + sequence_length + p : i + sequence_length + p + 6])\n",
    "            for p in prediction_steps\n",
    "        ]\n",
    "        y.append(future_values)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_future_sequences(data_normalized, sequence_length, prediction_steps, step_size)\n",
    "\n",
    "print(f\"\\n构造后的 X.shape = {X.shape}, y.shape = {y.shape}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5. 划分训练集和测试集\n",
    "# ------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False  # 时间序列通常不建议打乱\n",
    ")\n",
    "\n",
    "print(f\"训练集大小: {X_train.shape}, 测试集大小: {X_test.shape}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 6. 构建增强型 GRU 网络\n",
    "#    - 3 层 GRU + Dropout + BatchNorm\n",
    "# ------------------------------\n",
    "sequence_input = Input(shape=(sequence_length, len(final_features)))\n",
    "\n",
    "# 第1层 GRU\n",
    "x = GRU(128, activation='tanh', return_sequences=True)(sequence_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# 第2层 GRU\n",
    "x = GRU(64, activation='tanh', return_sequences=True)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# 第3层 GRU\n",
    "x = GRU(32, activation='tanh')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# 全连接层\n",
    "x = Dense(64, activation='swish')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# 输出层 - 预测多个时间步\n",
    "output = Dense(len(prediction_steps), activation='linear')(x)\n",
    "\n",
    "model = Model(inputs=sequence_input, outputs=output)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ------------------------------\n",
    "# 7. 训练策略优化 - 动态学习率 & 早停\n",
    "# ------------------------------\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 8. 训练模型\n",
    "# ------------------------------\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=42,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[lr_scheduler, early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 9. 测试集评估\n",
    "# ------------------------------\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"\\nTest Loss (MSE):\", test_loss)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 逐步评估每个预测步\n",
    "def evaluate_predictions(y_true, y_pred, steps):\n",
    "    print(\"\\nStep-wise Performance Evaluation:\")\n",
    "    for idx, step in enumerate(steps):\n",
    "        # 假设每个数据点间隔为10分钟\n",
    "        hours = (step * 10) // 60\n",
    "        minutes = (step * 10) % 60\n",
    "\n",
    "        true_step = y_true[:, idx]\n",
    "        pred_step = y_pred[:, idx]\n",
    "\n",
    "        mae = mean_absolute_error(true_step, pred_step)\n",
    "        r2 = r2_score(true_step, pred_step)\n",
    "        print(f\"预测 {hours}h{minutes:02d}min 后 -> MAE={mae:.4f}, R²={r2:.4f}\")\n",
    "\n",
    "    # 整体评估\n",
    "    overall_mae = mean_absolute_error(y_true, y_pred)\n",
    "    overall_r2 = r2_score(y_true.flatten(), y_pred.flatten())\n",
    "    print(\"\\nOverall Performance:\")\n",
    "    print(f\"Overall MAE: {overall_mae:.4f}\")\n",
    "    print(f\"Overall R²: {overall_r2:.4f}\")\n",
    "\n",
    "evaluate_predictions(y_test, y_pred, prediction_steps)\n",
    "\n",
    "# ------------------------------\n",
    "# 10. 模型保存\n",
    "# ------------------------------\n",
    "model.save('ph_prediction_gru_model.keras')\n"
   ],
   "id": "552724ce7adf2221",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "缺失情况：\n",
      "_id             0\n",
      "createdAt       0\n",
      "temperature     0\n",
      "pH              0\n",
      "conductivity    0\n",
      "oxygen          0\n",
      "ppm             0\n",
      "waterLevel      0\n",
      "pm25            0\n",
      "__v             0\n",
      "dtype: int64\n",
      "\n",
      "特征相关性：\n",
      "                   pH  temperature      hour  day_of_week  is_weekend\n",
      "pH           1.000000    -0.255652 -0.029854     0.044361    0.022740\n",
      "temperature -0.255652     1.000000 -0.000693     0.028069    0.048097\n",
      "hour        -0.029854    -0.000693  1.000000    -0.003893   -0.004632\n",
      "day_of_week  0.044361     0.028069 -0.003893     1.000000    0.787225\n",
      "is_weekend   0.022740     0.048097 -0.004632     0.787225    1.000000\n",
      "\n",
      "构造后的 X.shape = (3273, 12, 5), y.shape = (3273, 8)\n",
      "训练集大小: (2618, 12, 5), 测试集大小: (655, 12, 5)\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 12, 5)]           0         \n",
      "                                                                 \n",
      " gru_18 (GRU)                (None, 12, 128)           51840     \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 12, 128)          512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 12, 128)           0         \n",
      "                                                                 \n",
      " gru_19 (GRU)                (None, 12, 64)            37248     \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 12, 64)           256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 12, 64)            0         \n",
      "                                                                 \n",
      " gru_20 (GRU)                (None, 32)                9408      \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 8)                 520       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 102,024\n",
      "Trainable params: 101,576\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "Epoch 1/42\n",
      "82/82 [==============================] - 8s 25ms/step - loss: 0.1354 - val_loss: 0.0544 - lr: 0.0010\n",
      "Epoch 2/42\n",
      "82/82 [==============================] - 1s 14ms/step - loss: 0.0401 - val_loss: 0.0293 - lr: 0.0010\n",
      "Epoch 3/42\n",
      "82/82 [==============================] - 1s 14ms/step - loss: 0.0228 - val_loss: 0.0155 - lr: 0.0010\n",
      "Epoch 4/42\n",
      "82/82 [==============================] - 2s 24ms/step - loss: 0.0167 - val_loss: 0.0082 - lr: 0.0010\n",
      "Epoch 5/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0141 - val_loss: 0.0084 - lr: 0.0010\n",
      "Epoch 6/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0120 - val_loss: 0.0077 - lr: 0.0010\n",
      "Epoch 7/42\n",
      "82/82 [==============================] - 1s 14ms/step - loss: 0.0113 - val_loss: 0.0071 - lr: 0.0010\n",
      "Epoch 8/42\n",
      "82/82 [==============================] - 1s 14ms/step - loss: 0.0106 - val_loss: 0.0071 - lr: 0.0010\n",
      "Epoch 9/42\n",
      "82/82 [==============================] - 1s 14ms/step - loss: 0.0099 - val_loss: 0.0105 - lr: 0.0010\n",
      "Epoch 10/42\n",
      "82/82 [==============================] - 1s 14ms/step - loss: 0.0101 - val_loss: 0.0067 - lr: 0.0010\n",
      "Epoch 11/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0086 - val_loss: 0.0071 - lr: 0.0010\n",
      "Epoch 12/42\n",
      "82/82 [==============================] - 1s 14ms/step - loss: 0.0095 - val_loss: 0.0059 - lr: 0.0010\n",
      "Epoch 13/42\n",
      "82/82 [==============================] - 1s 14ms/step - loss: 0.0085 - val_loss: 0.0058 - lr: 0.0010\n",
      "Epoch 14/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0084 - val_loss: 0.0073 - lr: 0.0010\n",
      "Epoch 15/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0079 - val_loss: 0.0062 - lr: 0.0010\n",
      "Epoch 16/42\n",
      "82/82 [==============================] - 1s 14ms/step - loss: 0.0087 - val_loss: 0.0046 - lr: 0.0010\n",
      "Epoch 17/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0075 - val_loss: 0.0049 - lr: 0.0010\n",
      "Epoch 18/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0078 - val_loss: 0.0047 - lr: 0.0010\n",
      "Epoch 19/42\n",
      "82/82 [==============================] - 1s 16ms/step - loss: 0.0073 - val_loss: 0.0060 - lr: 0.0010\n",
      "Epoch 20/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0073 - val_loss: 0.0081 - lr: 0.0010\n",
      "Epoch 21/42\n",
      "82/82 [==============================] - 1s 16ms/step - loss: 0.0076 - val_loss: 0.0103 - lr: 0.0010\n",
      "Epoch 22/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0070 - val_loss: 0.0063 - lr: 5.0000e-04\n",
      "Epoch 23/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0063 - val_loss: 0.0040 - lr: 5.0000e-04\n",
      "Epoch 24/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0065 - val_loss: 0.0046 - lr: 5.0000e-04\n",
      "Epoch 25/42\n",
      "82/82 [==============================] - 1s 16ms/step - loss: 0.0067 - val_loss: 0.0062 - lr: 5.0000e-04\n",
      "Epoch 26/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0063 - val_loss: 0.0058 - lr: 5.0000e-04\n",
      "Epoch 27/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0064 - val_loss: 0.0052 - lr: 5.0000e-04\n",
      "Epoch 28/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0064 - val_loss: 0.0104 - lr: 5.0000e-04\n",
      "Epoch 29/42\n",
      "82/82 [==============================] - 1s 16ms/step - loss: 0.0065 - val_loss: 0.0042 - lr: 2.5000e-04\n",
      "Epoch 30/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0060 - val_loss: 0.0041 - lr: 2.5000e-04\n",
      "Epoch 31/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0061 - val_loss: 0.0041 - lr: 2.5000e-04\n",
      "Epoch 32/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0060 - val_loss: 0.0041 - lr: 2.5000e-04\n",
      "Epoch 33/42\n",
      "82/82 [==============================] - 1s 16ms/step - loss: 0.0059 - val_loss: 0.0039 - lr: 2.5000e-04\n",
      "Epoch 34/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0062 - val_loss: 0.0040 - lr: 1.2500e-04\n",
      "Epoch 35/42\n",
      "82/82 [==============================] - 1s 14ms/step - loss: 0.0057 - val_loss: 0.0051 - lr: 1.2500e-04\n",
      "Epoch 36/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0059 - val_loss: 0.0043 - lr: 1.2500e-04\n",
      "Epoch 37/42\n",
      "82/82 [==============================] - 1s 16ms/step - loss: 0.0058 - val_loss: 0.0044 - lr: 1.2500e-04\n",
      "Epoch 38/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0057 - val_loss: 0.0044 - lr: 1.2500e-04\n",
      "Epoch 39/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0057 - val_loss: 0.0042 - lr: 6.2500e-05\n",
      "Epoch 40/42\n",
      "82/82 [==============================] - 1s 16ms/step - loss: 0.0054 - val_loss: 0.0038 - lr: 6.2500e-05\n",
      "Epoch 41/42\n",
      "82/82 [==============================] - 1s 15ms/step - loss: 0.0058 - val_loss: 0.0040 - lr: 6.2500e-05\n",
      "Epoch 42/42\n",
      "82/82 [==============================] - 1s 16ms/step - loss: 0.0059 - val_loss: 0.0040 - lr: 6.2500e-05\n",
      "\n",
      "Test Loss (MSE): 0.0039658332243561745\n",
      "21/21 [==============================] - 1s 4ms/step\n",
      "\n",
      "Step-wise Performance Evaluation:\n",
      "预测 1h00min 后 -> MAE=0.0326, R²=0.9089\n",
      "预测 2h00min 后 -> MAE=0.0323, R²=0.9013\n",
      "预测 3h00min 后 -> MAE=0.0351, R²=0.8868\n",
      "预测 4h00min 后 -> MAE=0.0370, R²=0.8660\n",
      "预测 5h00min 后 -> MAE=0.0370, R²=0.8559\n",
      "预测 6h00min 后 -> MAE=0.0391, R²=0.8380\n",
      "预测 7h00min 后 -> MAE=0.0403, R²=0.8275\n",
      "预测 8h00min 后 -> MAE=0.0444, R²=0.8074\n",
      "\n",
      "Overall Performance:\n",
      "Overall MAE: 0.0372\n",
      "Overall R²: 0.8614\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "89104921d046773f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
